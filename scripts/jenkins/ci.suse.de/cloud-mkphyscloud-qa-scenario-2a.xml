<?xml version="1.0" encoding="UTF-8"?>
<project>
  <actions/>
  <description>This job will redeploy the qa2.cloud with scenario-2b:&#13;
 - total 8 nodes including admin node&#13;
 - database: default, postgresql&#13;
 - pacemaker: 3 clusters with 2 nodes (SBD)&#13;
 - keystone UUID&#13;
 - swift (allow public container, enable object versioning)&#13;
 - glance (storage swift)&#13;
 - nova: ssl, libvirt migration enabled, shared storage, kvm kernel samepage merging, 1 KVM&#13;
 - cinder: Local file&#13;
 - neutron: linuxbridge&#13;
 - everything with SSL.&#13;
&#13;
Warning:&#13;
It will wipe all machines!</description>
  <keepDependencies>false</keepDependencies>
  <properties>
    <hudson.security.AuthorizationMatrixProperty>
      <permission>hudson.scm.SCM.Tag:cloud</permission>
      <permission>hudson.model.Run.Delete:cloud</permission>
      <permission>hudson.model.Item.Read:anonymous</permission>
      <permission>hudson.model.Item.Read:cloud</permission>
      <permission>hudson.model.Item.Discover:cloud</permission>
      <permission>hudson.model.Item.Build:cloud</permission>
      <permission>hudson.model.Item.Cancel:cloud</permission>
      <permission>hudson.model.Item.Workspace:cloud</permission>
      <permission>hudson.model.Item.Delete:cloud</permission>
      <permission>hudson.model.Item.Configure:cloud</permission>
      <permission>hudson.model.Run.Update:cloud</permission>
    </hudson.security.AuthorizationMatrixProperty>
    <com.sonyericsson.jenkins.plugins.bfa.model.ScannerJobProperty plugin="build-failure-analyzer@">
      <doNotScan>false</doNotScan>
    </com.sonyericsson.jenkins.plugins.bfa.model.ScannerJobProperty>
    <com.sonyericsson.rebuild.RebuildSettings plugin="rebuild@">
      <autoRebuild>false</autoRebuild>
      <rebuildDisabled>false</rebuildDisabled>
    </com.sonyericsson.rebuild.RebuildSettings>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>cloudsource</name>
          <description/>
          <defaultValue>develcloud6</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>TESTHEAD</name>
          <description>if non-empty, test latest version from Devel:Cloud Staging</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>cct_tests</name>
          <description>Run cct</description>
          <defaultValue>features:base</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>want_ceph</name>
          <description>Install ceph barclamp</description>
          <defaultValue/>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>hw_number</name>
          <description>Name of the  QA cloud server (we have 2,3,4)</description>
          <defaultValue>2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>hacloud</name>
          <description>By default we don't want HA configured and installed</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>clusterconfig</name>
          <description>HA configuration for clusters. Make sense only if ha=1</description>
          <defaultValue>services=2,data=2,network=2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>nodenumber</name>
          <description>Default nodenumber is 7 as the qa2 cloud is the default here and it has 7 nodes. Cloud qa3 has 8 nodes</description>
          <defaultValue>7</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>want_ipmi</name>
          <description/>
          <defaultValue>true</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>runner_url</name>
          <description>The runner must be qa_crowbarsetup.sh file</description>
          <defaultValue>https://raw.github.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>commands</name>
          <description>All the steps that needs to be completed to have cloud installed</description>
          <defaultValue>prepareinstallcrowbar installcrowbar allocate setup_aliases waitcloud </defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>scenario_url</name>
          <description>Location of scenario YML file</description>
          <defaultValue>https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/scenarios/cloud6/qa-scenario-2a-sbd-kvm.yaml</defaultValue>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.scm.NullSCM"/>
  <assignedNode>cloud-mkphyscloud-gate-qa</assignedNode>
  <canRoam>false</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers/>
  <concurrentBuild>true</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>admin=crowbar$hw_number
cloud=qa$hw_number
tempestoptions="-t -s"


if [ ! -z "$UPDATEREPOS" ] ; then
  # testing update only makes sense with GMx and without TESTHEAD
  unset TESTHEAD
  export UPDATEREPOS=${UPDATEREPOS//$'\n'/+}
fi


export artifacts_dir=$WORKSPACE/.artifacts
rm -rf $artifacts_dir
mkdir -p $artifacts_dir
touch $artifacts_dir/.ignore

freshadminvm_cloud6 $admin qcow2
sleep 100 # time for the admin VM to boot

# wipe out shared NFS that should be used by this deployment

ssh root@10.162.26.129 "/var/qa-scripts/wipe_nfs_shares.sh qa2"

# rest of code runs on admin node:
env | grep -e networking -e libvirt -e cloud &gt; mkcloud.config
scp mkcloud.config root@$admin:
ret=0

ssh root@$admin "
export cloud=$cloud ; 
export UPDATEREPOS=$UPDATEREPOS ; 
export UPDATEBEFOREINSTALL=$UPDATEBEFOREINSTALL ;  
export TESTHEAD=$TESTHEAD ;
export cloudsource=$cloudsource ; 
export nodenumber=$nodenumber ; 
export hacloud=$hacloud ; 
export clusterconfig=$clusterconfig ; 
export runner_url=$runner_url ; 
export scenario_url=$scenario_url ; 
export want_node_aliases=controller=2,data=2,network=2,computekvm=1 ;
export want_node_roles=controller=2,storage=2,network=2,compute=1 ;
export scenario=\"scenario.yml\" ;
export commands=\"$commands\" "'


wget --no-check-certificate -Oqa_crowbarsetup.sh "$runner_url"

wget --no-check-certificate -Oscenario.yml "$scenario_url"

[ $UPDATEBEFOREINSTALL == "true" ] &amp;&amp; export updatesteps="addupdaterepo runupdate"

timeout --signal=ALRM 240m bash -x -c ". qa_crowbarsetup.sh ; onadmin_runlist $commands"
' || ret=$?

echo "mkphyscloud ret=$ret (before scenario)"

# FIXME qa_crowbarsetup.sh currently does not have testsetup and cct steps!! (because they could be called only after the deployment, which is bellow)

if [ "$ret" = "0" ]; then

  # ----- Prepare the SBD setup:
  
  cat &gt; /tmp/sbd_prepare_$admin &lt;&lt;EOSCRIPT
  
# preparation of iSCSI
zypper --gpg-auto-import-keys -p http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12_SP1/ --non-interactive install python-sh


wget --no-check-certificate https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/iscsictl.py

chmod +x iscsictl.py

./iscsictl.py --service target --host \$(hostname) --no-key

./iscsictl.py --service initiator --target_host \$(hostname) --host controller1 --no-key
./iscsictl.py --service initiator --target_host \$(hostname) --host controller2 --no-key

./iscsictl.py --service initiator --target_host \$(hostname) --host data1 --no-key
./iscsictl.py --service initiator --target_host \$(hostname) --host data2 --no-key

./iscsictl.py --service initiator --target_host \$(hostname) --host network1 --no-key
./iscsictl.py --service initiator --target_host \$(hostname) --host network2 --no-key



# preparation of SBD for services nodes
SBD_DEV_SERVICES=\$(ssh controller1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
ssh controller1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_SERVICES create"
ssh controller2 "zypper --non-interactive install sbd"
# take scenario yaml file and replace placeholders
sed -i "s|@@sbd_device_services@@|\${SBD_DEV_SERVICES}|g" scenario.yml

# preparation of SBD for data nodes
SBD_DEV_DATA=\$(ssh data1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
ssh data1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_DATA create"
ssh data2 "zypper --non-interactive install sbd"
# take scenario yaml file and replace placeholders
sed -i "s|@@sbd_device_data@@|\${SBD_DEV_DATA}|g" scenario.yml

# preparation of SBD for network nodes
SBD_DEV_NETWORK=\$(ssh network1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
ssh network1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV_NETWORK create"
ssh network2 "zypper --non-interactive install sbd"
# take scenario yaml file and replace placeholders
sed -i "s|@@sbd_device_network@@|\${SBD_DEV_NETWORK}|g" scenario.yml

# watchdog configuration
ssh controller1 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
ssh controller2 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf

ssh data1 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
ssh data2 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf

ssh network1 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
ssh network2 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf


# ----- End of SBD
EOSCRIPT

  chmod +x /tmp/sbd_prepare_$admin
  
  scp /tmp/sbd_prepare_$admin root@$admin:sbd_prepare
  

ssh root@$admin  "
  export cloud=$cloud ; 
  export TESTHEAD=$TESTHEAD ;
  export cloudsource=$cloudsource ; 
  export nodenumber=$nodenumber ; 
  export hacloud=$hacloud ; 
  export clusterconfig=$clusterconfig ; 
  export want_ceph=1 ;
  export cephvolumenumber=0;
  export cct_tests=$cct_tests;
  export scenario=$scenario "'
  
  ./sbd_prepare
   
  # crowbar_batch --timeout 2400 build scenario.yml
  
  timeout --signal=ALRM 60m bash -x -c ". qa_crowbarsetup.sh ; onadmin_batch;"
  
  timeout --signal=ALRM 60m bash -x -c ". qa_crowbarsetup.sh ; onadmin_runlist testsetup; onadmin_run_cct;"
  ' || ret=$?

echo "mkphyscloud ret=$ret"

fi

exit $ret




</command>
    </hudson.tasks.Shell>
  </builders>
  <publishers>
    <hudson.tasks.ArtifactArchiver>
      <artifacts>.artifacts/**</artifacts>
      <allowEmptyArchive>false</allowEmptyArchive>
      <onlyIfSuccessful>false</onlyIfSuccessful>
      <fingerprint>false</fingerprint>
      <defaultExcludes>true</defaultExcludes>
      <caseSensitive>true</caseSensitive>
    </hudson.tasks.ArtifactArchiver>
    <hudson.tasks.Mailer plugin="mailer@">
      <recipients/>
      <dontNotifyEveryUnstableBuild>true</dontNotifyEveryUnstableBuild>
      <sendToIndividuals>false</sendToIndividuals>
    </hudson.tasks.Mailer>
  </publishers>
  <buildWrappers>
    <hudson.plugins.timestamper.TimestamperBuildWrapper plugin="timestamper@"/>
    <org.jenkinsci.plugins.buildnamesetter.BuildNameSetter plugin="build-name-setter@">
      <template>#${BUILD_NUMBER}: ${ENV,var="cloudsource"} (${ENV,var="nodenumber"}/${ENV,var="tempestoptions"}/${ENV,var="mkcloudtarget"} ${ENV,var="hacloud"})</template>
    </org.jenkinsci.plugins.buildnamesetter.BuildNameSetter>
  </buildWrappers>
</project>
