<?xml version="1.0" encoding="UTF-8"?>
<project>
  <actions/>
  <description>&lt;b&gt;This job is managed by JJB! Changes must be done in &lt;a href='https://github.com/SUSE-Cloud/automation/tree/master/scripts/jenkins/jobs-ibs/'&gt;git&lt;/a&gt;&lt;/b&gt;

This job will redeploy the qa2.cloud with scenario-2b:
  - total 7 nodes
  - database: default, postgresql
  - pacemaker: 1 cluster with 3 controllers
  - nova: ssl, libvirt migration enabled, shared storage, kvm kernel samepage merging, 2 kvm and 2 ESXi compute nodes
  - cinder: vmware
  - neutron: vmware (but currently ovs as NSX plugin is kind of out of order)
  - eveyrthing with SSL.

Warning: It will wipe all machines!
&lt;!-- Managed by Jenkins Job Builder --&gt;</description>
  <keepDependencies>false</keepDependencies>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <concurrentBuild>false</concurrentBuild>
  <assignedNode>cloud-mkphyscloud-gate-qa</assignedNode>
  <canRoam>false</canRoam>
  <logRotator>
    <daysToKeep>-1</daysToKeep>
    <numToKeep>7</numToKeep>
    <artifactDaysToKeep>-1</artifactDaysToKeep>
    <artifactNumToKeep>-1</artifactNumToKeep>
  </logRotator>
  <properties>
    <hudson.model.ParametersDefinitionProperty>
      <parameterDefinitions>
        <hudson.model.StringParameterDefinition>
          <name>cloudsource</name>
          <description/>
          <defaultValue>susecloud6</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>TESTHEAD</name>
          <description>if non-empty, test latest version from Devel:Cloud Stagin</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>cct_tests</name>
          <description>Run cct</description>
          <defaultValue>features:base</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>want_ceph</name>
          <description>Install ceph barclamp</description>
          <defaultValue/>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>hw_number</name>
          <description>Name of the QA cloud server (we have 2,3,4)</description>
          <defaultValue>2</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>hacloud</name>
          <description>By default we do not want HA configured and installed</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>clusterconfig</name>
          <description>HA configuration for clusters. Make sense only if hacloud=1</description>
          <defaultValue>services=3</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>nodenumber</name>
          <description>Number of nodes to use. Depends on hw_number</description>
          <defaultValue>7</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>want_ipmi</name>
          <description/>
          <defaultValue>true</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>runner_url</name>
          <description>The runner must be qa_crowbarsetup.sh file</description>
          <defaultValue>https://raw.github.com/SUSE-Cloud/automation/master/scripts/qa_crowbarsetup.sh</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>commands</name>
          <description>All the steps that needs to be completed to have cloud installed</description>
          <defaultValue>prepareinstallcrowbar installcrowbar allocate setup_aliases waitcloud</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>scenario_url</name>
          <description>Location of scenario yaml file</description>
          <defaultValue>https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/scenarios/cloud6/qa/no-ssl/qa-scenario-2b-sbd-kvm-vmware.yaml</defaultValue>
        </hudson.model.StringParameterDefinition>
        <hudson.model.StringParameterDefinition>
          <name>wanttempest</name>
          <description>Run tempest smoke tests</description>
          <defaultValue>1</defaultValue>
        </hudson.model.StringParameterDefinition>
      </parameterDefinitions>
    </hudson.model.ParametersDefinitionProperty>
  </properties>
  <scm class="hudson.scm.NullSCM"/>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash
admin=crowbar$hw_number
cloud=qa$hw_number
tempestoptions="-t -s"
vcenter_user="Administrator@vsphere.qa.suse.de"
vcenter_password=`cat /home/jenkins/passwords/Administrator\@vsphere.qa.suse.de`

if [ ! -z "$UPDATEREPOS" ] ; then
  # testing update only makes sense with GMx and without TESTHEAD
  unset TESTHEAD
  export UPDATEREPOS=${UPDATEREPOS//$'\n'/+}
fi

export artifacts_dir=$WORKSPACE/.artifacts
rm -rf $artifacts_dir
mkdir -p $artifacts_dir
touch $artifacts_dir/.ignore

freshadminvm_cloud6 $admin qcow2
sleep 100 # time for the admin VM to boot

# wipe out shared NFS that should be used by this deployment
ssh root@10.162.26.129 "/var/qa-scripts/wipe_nfs_shares.sh qa$hw_number"

# rest of code runs on admin node:
env | grep -e networking -e libvirt -e cloud &gt; mkcloud.config
scp mkcloud.config root@$admin:
ret=0

ssh root@$admin "
export cloud=$cloud ;
export UPDATEREPOS=$UPDATEREPOS ;
export UPDATEBEFOREINSTALL=$UPDATEBEFOREINSTALL ;
export TESTHEAD=$TESTHEAD ;
export cloudsource=$cloudsource ;
export nodenumber=$nodenumber ;
export hacloud=$hacloud ;
export clusterconfig=$clusterconfig ;
export runner_url=$runner_url ;
export scenario_url=$scenario_url ;
export want_node_aliases=controller=3,computekvm=2,computevmw=1 ;
export want_node_roles=controller=3,compute=4 ;
export wanttempest=$wanttempest;
export tempestoptions=\"$tempestoptions\";
export scenario=\"scenario.yml\" ;
export commands=\"$commands\" "'

wget --no-check-certificate -Oqa_crowbarsetup.sh "$runner_url"
wget --no-check-certificate -Oscenario.yml "$scenario_url"

sed -i -e "s,##shared_nfs_for_database##,10.162.26.129:/var/$cloud/ha-database," scenario.yml
sed -i -e "s,##shared_nfs_for_rabbitmq##,10.162.26.129:/var/$cloud/ha-rabbitmq," scenario.yml

[ $UPDATEBEFOREINSTALL == "true" ] &amp;&amp; export updatesteps="addupdaterepo runupdate"

timeout --signal=ALRM 240m bash -x -c ". qa_crowbarsetup.sh ; onadmin_runlist $commands"
' || ret=$?

echo "mkphyscloud ret=$ret (before scenario)"

if [ "$ret" = "0" ]; then
  # ----- Prepare the SBD setup:
  cat &gt; /tmp/sbd_prepare_$admin &lt;&lt;EOSCRIPT
    # preparation of iSCSI
    zypper --gpg-auto-import-keys -p http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12_SP1/ --non-interactive install python-sh
    wget --no-check-certificate https://raw.githubusercontent.com/SUSE-Cloud/automation/master/scripts/iscsictl.py
    chmod +x iscsictl.py
    ./iscsictl.py --service target --host \$(hostname) --no-key
    ./iscsictl.py --service initiator --target_host \$(hostname) --host controller1 --no-key
    ./iscsictl.py --service initiator --target_host \$(hostname) --host controller2 --no-key
    ./iscsictl.py --service initiator --target_host \$(hostname) --host controller3 --no-key
    # preparation of SBD
    SBD_DEV=\$(ssh controller1 echo '/dev/disk/by-id/scsi-\$(lsscsi -i | grep LIO | tr -s " " |cut -d " " -f7)')
    ssh controller1 "zypper --non-interactive install sbd; sbd -d \$SBD_DEV create"
    ssh controller2 "zypper --non-interactive install sbd"
    ssh controller3 "zypper --non-interactive install sbd"
    # watchdog configuration
    ssh controller1 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
    ssh controller2 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
    ssh controller3 modprobe softdog; echo softdog &gt; /etc/modules-load.d/watchdog.conf
    # take scenario yaml file and replace placeholders with right things:
    sed -i "s|@@sbd_device@@|\${SBD_DEV}|g" scenario.yml
    sed -i "s|@@vcenter_user@@|\${vcenter_user}|g" scenario.yml
    sed -i "s|@@vcenter_password@@|\${vcenter_password}|g" scenario.yml
    # ----- End of SBD
  EOSCRIPT
  chmod +x /tmp/sbd_prepare_$admin
  scp /tmp/sbd_prepare_$admin root@$admin:sbd_prepare
  ssh root@$admin './sbd_prepare
  ' || ret=$?
fi
if [ "$ret" = "0" ]; then
  echo "Running testsetup from admin node..."
  ssh root@$admin "
  export cloud=$cloud ;
  export TESTHEAD=$TESTHEAD ;
  export cloudsource=$cloudsource ;
  export nodenumber=$nodenumber ;
  export hacloud=$hacloud ;
  export clusterconfig=$clusterconfig ;
  export want_ceph=1 ;
  export cephvolumenumber=2;
  export cct_tests=$cct_tests;
  export wanttempest=$wanttempest;
  export tempestoptions=\"$tempestoptions\";
  export scenario=scenario.yml "'

  timeout --signal=ALRM 240m bash -x -c ". qa_crowbarsetup.sh ; onadmin_runlist batch testsetup;"
  ' || ret=$?


  if [ $ret != 0 ] ; then
    ssh root@$admin '
    set -x
    for node in $(crowbar machines list | grep ^d) ; do
      (
      echo "Collecting supportconfig from $node"
      timeout 400 ssh $node supportconfig | wc
      timeout 300 scp $node:/var/log/\*tbz /var/log/
      )&amp;
    done
    timeout 500 supportconfig | wc &amp;
    wait
    '

    scp root@$admin:/var/log/*tbz $artifacts_dir/
  fi &gt;&amp;2
  echo "mkphyscloud ret=$ret"
fi
exit $ret
</command>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers>
    <org.jenkinsci.plugins.buildnamesetter.BuildNameSetter>
      <template/>
    </org.jenkinsci.plugins.buildnamesetter.BuildNameSetter>
    <hudson.plugins.timestamper.TimestamperBuildWrapper/>
  </buildWrappers>
</project>
